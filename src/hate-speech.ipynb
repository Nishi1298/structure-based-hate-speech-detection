{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Based Hate Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter=',')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    #pass\n",
    "    data = [x[6] for x in raw]\n",
    "    return np.array(data)\n",
    "\n",
    "def getClass(raw):\n",
    "    #pass\n",
    "    classes = [x[5] for x in raw]\n",
    "    return np.array(classes)\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tokenize(tweet)\n",
    "#         print(tweet)\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "#     print(text)\n",
    "    return text.split()\n",
    "    #return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "    print(\"F1 score:   \", f1)\n",
    "    print(\"Avg Recall: \", rec)    \n",
    "    print(\"Accuracy:   \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = DATA_PATH + \"labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "ds = pd.read_csv(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in ds.iterrows(): \n",
    "    print(index)\n",
    "    x = np.argmax([row['hate_speech'], row['offensive_language'], row['neither']])\n",
    "    if x == 0:\n",
    "        ds.loc[index, 'hate_speech'] = 1\n",
    "        ds.loc[index, 'offensive_language'] = 0\n",
    "        ds.loc[index, 'neither'] = 0\n",
    "#         row['offensive_language']=0 \n",
    "#         row['neither'] = 0\n",
    "    elif x == 1:\n",
    "        ds.loc[index, 'hate_speech'] = 0\n",
    "        ds.loc[index, 'offensive_language'] = 1\n",
    "        ds.loc[index, 'neither'] = 0\n",
    "#         row['hate_speech'] = 0\n",
    "        row['offensive_language']=1 \n",
    "        row['neither'] = 0\n",
    "    elif x == 2:\n",
    "        ds.loc[index, 'hate_speech'] = 0\n",
    "        ds.loc[index, 'offensive_language'] = 0\n",
    "        ds.loc[index, 'neither'] = 1\n",
    "#         row['hate_speech'] = 0\n",
    "#         row['offensive_language']=0 \n",
    "#         row['neither'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(DATA_PATH + \"labeled_data-mod.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw = readData(DATA) \n",
    "tweets = getTweets(raw)\n",
    "classes = getClass(raw)\n",
    "tweets = preprocess(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x in tweets]\n",
    "X = np.delete(np.array(X), [0])\n",
    "y = np.delete(classes, [0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# class LSTMClassifier(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embedding_dim, hidden_dim, output_size, batch_size, num_layers = 1):\n",
    "\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.output_size = output_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "#         # Naive embeddings for testing purposes\n",
    "#         self.embedding = nn.Embedding(1024, embedding_dim)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers)\n",
    "#         self.hidden2out = nn.Linear(hidden_dim, output_size)\n",
    "                \n",
    "#         self.hidden = self.init_hidden()\n",
    "#         self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "#         self.dropout_layer = nn.Dropout(p = 0.2)\n",
    "    \n",
    "#     def init_hidden(self):\n",
    "#          return (autograd.Variable(torch.randn(self.num_layers, self.batch_size, self.hidden_dim)),\n",
    "#                 autograd.Variable(torch.randn(self.num_layers, self.batch_size, self.hidden_dim)))\n",
    "        \n",
    "#     def forward(self, sents, lengths):\n",
    "#         embeds = self.embedding(sents)\n",
    "#         packed_input = pack_padded_sequence(embeds, lengths)\n",
    "        \n",
    "#         lstm_out, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "#         y = self.hidden2out(lstm_out[-1])\n",
    "#         y = self.softmax(y)\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMClassifier(128, 32, 2, 1)\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "# model(X, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset\n",
    "def cust_preprocess(tweet):\n",
    "    tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "    tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "    tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "    tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "    tweet = re.sub(\" +\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tokenize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, tokenize = cust_preprocess, lower=True)\n",
    "LABEL = Field(sequential = False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"ct\", None),\n",
    "                 (\"count\", None),\n",
    "                 (\"hate_speech\", LABEL),\n",
    "                 (\"offensive\", LABEL),\n",
    "                 (\"neither\", LABEL),\n",
    "                 (\"label\", None),\n",
    "                 (\"tweet\", TEXT)]\n",
    "\n",
    "dt = TabularDataset(\n",
    "               path=DATA_PATH + \"labeled_data-mod.csv\",\n",
    "               format='csv',\n",
    "               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields)\n",
    "trn, dev,tst = dt.split([0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, dev_iter = BucketIterator.splits(\n",
    "     (trn, dev), # we pass in the datasets we want the iterator to draw data from\n",
    "     batch_sizes=(64, 64),\n",
    "     device=device, # if you want to use the GPU, specify the GPU number here\n",
    "     sort_key=lambda x: len(x.tweet), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "     sort_within_batch=False,\n",
    "     repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "test_iter = Iterator(tst, batch_size=64, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])\n",
    "test_dl = BatchWrapper(test_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])\n",
    "dev_dl = BatchWrapper(dev_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trip3r/venv/lib/python3.5/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleBiLSTMBaseline(\n",
       "  (embedding): Embedding(31438, 100)\n",
       "  (encoder): LSTM(100, 500, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=500, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMBaseline(nh, emb_dim=em_sz)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:05<00:00, 60.41it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 81.53it/s]\n",
      "  1%|▏         | 4/310 [00:00<00:09, 32.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.1853, Validation Loss: 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:05<00:00, 47.79it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 131.34it/s]\n",
      "  3%|▎         | 10/310 [00:00<00:03, 93.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.1470, Validation Loss: 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:05<00:00, 57.80it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 446.12it/s]\n",
      "  3%|▎         | 8/310 [00:00<00:04, 67.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.1003, Validation Loss: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 43/310 [00:00<00:06, 44.40it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "# opt = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm.tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm.tqdm(dev_dl):\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(dev)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    for tt in tqdm.tqdm(test_iter):\n",
    "        preds = model(tt.tweet)\n",
    "#         print(tt.tweet)\n",
    "        preds = preds.cpu()\n",
    "        preds = preds.data.numpy()\n",
    "        preds = preds/preds.max()\n",
    "        \n",
    "        # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n",
    "        preds = 1 / (1 + np.exp(-preds))\n",
    "#         print(preds)\n",
    "        test_preds.append(preds)\n",
    "#     test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1737.7166, 1942.4241, 1870.5917],[1,2,3]])\n",
    "x - np.mean(x, axis=1).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
