{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Based Hate Speech Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(path):\n",
    "    data = []\n",
    "    with open(path,'r') as file:\n",
    "        data = [x for x in csv.reader(file, delimiter=',')]\n",
    "    return data\n",
    "\n",
    "def getTweets(raw):\n",
    "    #pass\n",
    "    data = [x[6] for x in raw]\n",
    "    return np.array(data)\n",
    "\n",
    "def getClass(raw):\n",
    "    #pass\n",
    "    classes = [x[5] for x in raw]\n",
    "    return np.array(classes)\n",
    "\n",
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(data):\n",
    "    cleanData = []\n",
    "    for tweet in data:\n",
    "        tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "        tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "        tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "        tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "        tweet = re.sub(\" +\", \" \", tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = tokenize(tweet)\n",
    "#         print(tweet)\n",
    "        cleanData.append(tweet)\n",
    "    return cleanData\n",
    "\n",
    "def tokenize(text):\n",
    "#     print(text)\n",
    "    return text.split()\n",
    "    #return TweetTokenizer.tokenize(text)\n",
    "\n",
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='weighted')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    rec = recall_score(target, predicted, average = 'macro')\n",
    "#     print(\"F1 score:   \", f1)\n",
    "#     print(\"Avg Recall: \", rec)    \n",
    "#     print(\"Accuracy:   \", acc*100)\n",
    "    return f1,acc*100,rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA = DATA_PATH + \"labeled_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "raw = readData(DATA) \n",
    "r_tweets = getTweets(raw)\n",
    "r_tweets = r_tweets[1:len(r_tweets)]\n",
    "classes = getClass(raw)\n",
    "classes = classes[1:len(classes)]\n",
    "tweets = preprocess(r_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##for pos part\n",
    "fd = pd.read_csv(\"../data/labeled_data.csv\")\n",
    "all_tweets = fd.tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = [x for x in r_tweets]\n",
    "# X = np.delete(np.array(X), [0])\n",
    "# y = np.delete(classes, [0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for t in tweets:\n",
    "    x = ' '.join(t)\n",
    "    data.append(x)\n",
    "Y = classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LR(train_features,test_features,y_train,y_test,string):\n",
    "    classifier = LogisticRegression(random_state=0, solver='lbfgs')   \n",
    "    classifier.fit(train_features, y_train)\n",
    "    y_predict = classifier.predict(test_features)\n",
    "    f1,acc,rec=evaluate(y_test, y_predict)\n",
    "    ans['model'].append(string)\n",
    "    ans['F1-score'].append(f1)\n",
    "    ans['Recall'].append(rec)\n",
    "    ans['Accuracy'].append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM(train_features,test_features,y_train,y_test,string):\n",
    "    classifier = SVC(C = 1,kernel='rbf')\n",
    "    classifier.fit(train_features, y_train)\n",
    "    y_predict = classifier.predict(test_features)\n",
    "    f1,acc,rec=evaluate(y_test, y_predict)\n",
    "    ans['model'].append(string)\n",
    "    ans['F1-score'].append(f1)\n",
    "    ans['Recall'].append(rec)\n",
    "    ans['Accuracy'].append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Tokens with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(1,1),(1,2),(1,3)]\n",
    "keys = ['unigram','bigram','trigram']\n",
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []\n",
    "for p in range(len(params)):\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer = 'word',\n",
    "        lowercase = True,\n",
    "        tokenizer = tokenize,\n",
    "        ngram_range=params[p],\n",
    "        stop_words = en_stopwords)\n",
    "    vectorizer.fit(X_train)\n",
    "    train_features = vectorizer.transform(X_train)\n",
    "    test_features = vectorizer.transform(X_test)\n",
    "    string = keys[p]+' '+'using LR'\n",
    "    LR(train_features,test_features,y_train,y_test,string)\n",
    "    string = keys[p]+' '+'using SVM'\n",
    "    SVM(train_features,test_features,y_train,y_test,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.380069</td>\n",
       "      <td>0.871655</td>\n",
       "      <td>0.650787</td>\n",
       "      <td>unigram using LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.538229</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>unigram using SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.117813</td>\n",
       "      <td>0.866663</td>\n",
       "      <td>0.634880</td>\n",
       "      <td>bigram using LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.538229</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>bigram using SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.936252</td>\n",
       "      <td>0.863420</td>\n",
       "      <td>0.624250</td>\n",
       "      <td>trigram using LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.538229</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>trigram using SVM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall              model\n",
       "0  88.380069  0.871655  0.650787   unigram using LR\n",
       "1  76.538229  0.663664  0.333333  unigram using SVM\n",
       "2  88.117813  0.866663  0.634880    bigram using LR\n",
       "3  76.538229  0.663664  0.333333   bigram using SVM\n",
       "4  87.936252  0.863420  0.624250   trigram using LR\n",
       "5  76.538229  0.663664  0.333333  trigram using SVM"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-level Tokens with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'char',\n",
    "    lowercase = True,\n",
    "    tokenizer = tokenize,\n",
    "    ngram_range=(2, 6),\n",
    "    stop_words = en_stopwords)\n",
    "vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []\n",
    "string = 'Char level using LR'\n",
    "LR(train_features,test_features,y_train,y_test,string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Char level using SVM'\n",
    "SVM(train_features,test_features,y_train,y_test,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.792213</td>\n",
       "      <td>0.890695</td>\n",
       "      <td>0.707550</td>\n",
       "      <td>Char level using LR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.861005</td>\n",
       "      <td>0.668052</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Char level using SVM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall                 model\n",
       "0  89.792213  0.890695  0.707550   Char level using LR\n",
       "1  76.861005  0.668052  0.333333  Char level using SVM"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset\n",
    "def cust_preprocess(tweet):\n",
    "    tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "    tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "    tweet = tweet.replace(\"[^a-zA-Z#]\", \" \") # Removing punctuation and special characters\n",
    "    tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"<URL>\", tweet)\n",
    "    tweet = re.sub(\" +\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tokenize(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, tokenize = cust_preprocess, lower=True)\n",
    "LABEL = Field(sequential = False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"ct\", None),\n",
    "                 (\"count\", None),\n",
    "                 (\"hate_speech\", LABEL),\n",
    "                 (\"offensive\", LABEL),\n",
    "                 (\"neither\", LABEL),\n",
    "                 (\"label\", None),\n",
    "                 (\"tweet\", TEXT)]\n",
    "\n",
    "dt = TabularDataset(\n",
    "               path=DATA_PATH + \"labeled_data-mod.csv\",\n",
    "               format='csv',\n",
    "               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields)\n",
    "trn, dev,tst = dt.split([0.8,0.1,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, dev_iter = BucketIterator.splits(\n",
    "     (trn, dev), # we pass in the datasets we want the iterator to draw data from\n",
    "     batch_sizes=(64, 64),\n",
    "     device=device, # if you want to use the GPU, specify the GPU number here\n",
    "     sort_key=lambda x: len(x.tweet), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "     sort_within_batch=False,\n",
    "     repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "test_iter = Iterator(tst, batch_size=64, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "            if self.y_vars is not None: # we will concatenate y into a single tensor\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])\n",
    "test_dl = BatchWrapper(test_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])\n",
    "dev_dl = BatchWrapper(dev_iter, \"tweet\", [\"hate_speech\",\"offensive\",\"neither\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__() # don't forget to call this!\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 128-dim embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(13794, 128)\n",
       "  (encoder): LSTM(128, 250, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=250, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 128\n",
    "nh = 250\n",
    "nl = 3\n",
    "model = LSTMBaseline(nh, emb_dim=em_sz)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:50<00:00,  6.13it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 69.26it/s] \n",
      "  0%|          | 1/310 [00:00<00:54,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.1853, Validation Loss: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:41<00:00,  7.43it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 69.63it/s] \n",
      "  0%|          | 1/310 [00:00<00:52,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.1839, Validation Loss: 0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:47<00:00,  6.48it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 68.11it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.1555, Validation Loss: 0.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:01<00:00,  5.01it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 69.13it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.0945, Validation Loss: 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:04<00:00,  4.78it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 68.26it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.0816, Validation Loss: 0.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:18<00:00,  3.95it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 53.90it/s]\n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.0734, Validation Loss: 0.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:44<00:00,  2.97it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 51.15it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.0674, Validation Loss: 0.0558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:51<00:00,  2.78it/s]\n",
      "100%|██████████| 39/39 [00:01<00:00, 37.41it/s]\n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.0622, Validation Loss: 0.0547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:52<00:00,  2.76it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 81.92it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.0589, Validation Loss: 0.0551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [01:22<00:00,  3.75it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 54.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.0549, Validation Loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm(dev_dl):\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(dev)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 26.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_iter.\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    ground_truth = []\n",
    "    for x,y in tqdm(test_dl):\n",
    "        preds = model(x)\n",
    "        preds = F.softmax(preds)\n",
    "        preds = preds.cpu()\n",
    "        preds = preds.data.numpy()\n",
    "        for result in preds:\n",
    "            if np.argmax(result) == 0:\n",
    "                test_preds.append([1, 0, 0])\n",
    "            elif np.argmax(result) == 1:\n",
    "                test_preds.append([0, 1, 0])\n",
    "            elif np.argmax(result) == 2:\n",
    "                test_preds.append([0, 0, 1])\n",
    "        for val in y:\n",
    "            ground_truth.append(val.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.array(test_preds)\n",
    "ground_truth = np.array(ground_truth)\n",
    "f1,acc,re = evaluate(ground_truth, test_preds)\n",
    "string = 'LSTM 128-dim embedding'\n",
    "ans['model'].append(string)\n",
    "ans['F1-score'].append(f1)\n",
    "ans['Recall'].append(re)\n",
    "ans['Accuracy'].append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 256-dim embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(13794, 256)\n",
       "  (encoder): LSTM(256, 250, dropout=0.1)\n",
       "  (linear_layers): ModuleList()\n",
       "  (predictor): Linear(in_features=250, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 256\n",
    "nh = 250\n",
    "nl = 3\n",
    "model = LSTMBaseline(nh, emb_dim=em_sz)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [03:16<00:00,  1.57it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 74.20it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.1856, Validation Loss: 0.0941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [03:22<00:00,  1.53it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 72.46it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.1862, Validation Loss: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:51<00:00,  1.80it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 74.74it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.1667, Validation Loss: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:11<00:00,  2.36it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 62.41it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.1029, Validation Loss: 0.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:32<00:00,  2.03it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 51.12it/s]\n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.0835, Validation Loss: 0.0548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:42<00:00,  1.91it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 58.30it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.0756, Validation Loss: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:59<00:00,  1.72it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 69.20it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.0704, Validation Loss: 0.0610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [02:56<00:00,  1.76it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 72.95it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.0648, Validation Loss: 0.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [03:00<00:00,  1.72it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 54.58it/s] \n",
      "  0%|          | 0/310 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.0609, Validation Loss: 0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [03:00<00:00,  1.72it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 64.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.0561, Validation Loss: 0.0655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() # turn on training mode\n",
    "    for x,y in tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model.eval() # turn on evaluation mode\n",
    "    for x,y in tqdm(dev_dl):\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.data.item() * x.size(0)\n",
    "\n",
    "    val_loss /= len(dev)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 23.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_iter.\n",
    "with torch.no_grad():\n",
    "    test_preds = []\n",
    "    ground_truth = []\n",
    "    for x,y in tqdm(test_dl):\n",
    "        preds = model(x)\n",
    "        preds = F.softmax(preds)\n",
    "        preds = preds.cpu()\n",
    "        preds = preds.data.numpy()\n",
    "        for result in preds:\n",
    "            if np.argmax(result) == 0:\n",
    "                test_preds.append([1, 0, 0])\n",
    "            elif np.argmax(result) == 1:\n",
    "                test_preds.append([0, 1, 0])\n",
    "            elif np.argmax(result) == 2:\n",
    "                test_preds.append([0, 0, 1])\n",
    "        for val in y:\n",
    "            ground_truth.append(val.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.array(test_preds)\n",
    "ground_truth = np.array(ground_truth)\n",
    "f1,acc,rec = evaluate(ground_truth, test_preds)\n",
    "ans['model'].append(string)\n",
    "ans['F1-score'].append(f1)\n",
    "ans['Recall'].append(rec)\n",
    "ans['Accuracy'].append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.902341</td>\n",
       "      <td>0.882461</td>\n",
       "      <td>0.683782</td>\n",
       "      <td>LSTM with 128-dim embedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87.691687</td>\n",
       "      <td>0.882461</td>\n",
       "      <td>0.641605</td>\n",
       "      <td>LSTM with 256-dim embedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall                        model\n",
       "0  88.902341  0.882461  0.683782  LSTM with 128-dim embedding\n",
       "1  87.691687  0.882461  0.641605  LSTM with 256-dim embedding"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans['model'] = []\n",
    "string = 'LSTM with 256-dim embedding'\n",
    "ans['model'].append('LSTM with 128-dim embedding')\n",
    "ans['model'].append(string)\n",
    "ans['F1-score'] = ans['F1-score'][0:2]\n",
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using surface features,linguistic features and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def preprocessing(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = fd['class'].astype(int)\n",
    "ans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LR_l2(X_train,X_test,y_train,y_test,string):\n",
    "    LR2 = LogisticRegression(random_state = 0,class_weight='balanced',penalty='l2',solver='lbfgs')\n",
    "    LR2.fit(X_train, y_train)\n",
    "    y_predict = LR2.predict(X_test)\n",
    "    f1,acc,rec=evaluate(y_test, y_predict)\n",
    "    ans['model'].append(string)\n",
    "    ans['F1-score'].append(f1)\n",
    "    ans['Recall'].append(rec)\n",
    "    ans['Accuracy'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LR_l1(X_train,X_test,y_train,y_test,string):\n",
    "    LR1 = LogisticRegression(random_state = 0,class_weight='balanced',penalty=\"l1\", C=0.1)\n",
    "    LR1.fit(X_train, y_train)\n",
    "    y_predict = LR1.predict(X_test)\n",
    "    f1,acc,rec = evaluate(y_test, y_predict)\n",
    "    ans['model'].append(string)\n",
    "    ans['F1-score'].append(f1)\n",
    "    ans['Recall'].append(rec)\n",
    "    ans['Accuracy'].append(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams each weighted by Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_gram(param,string):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize,\n",
    "        preprocessor=preprocessing,\n",
    "        ngram_range=(param),\n",
    "        stop_words=stopwords,\n",
    "        use_idf=True,\n",
    "        smooth_idf=False,\n",
    "        norm=None,\n",
    "        decode_error='replace',\n",
    "        max_features=10000,\n",
    "        min_df=5,\n",
    "        max_df=0.75\n",
    "        )\n",
    "    #Construct tfidf matrix and get relevant scores\n",
    "    tfidf = vectorizer.fit_transform(all_tweets).toarray()\n",
    "    vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "    idf_vals = vectorizer.idf_\n",
    "    idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF score\n",
    "    X = pd.DataFrame(tfidf)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    string1 = 'LR with L1 norm on ' + string\n",
    "    LR_l1(X_train,X_test,y_train,y_test,string1)\n",
    "    string2  = 'LR with L2 norm on ' + string\n",
    "    LR_l2(X_train,X_test,y_train,y_test,string2)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []\n",
    "tfidf1=tf_idf_gram((1,1),'unigram')\n",
    "tfidf2=tf_idf_gram((1,2),'digram')\n",
    "tfidf3=tf_idf_gram((1,3),'trigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.570305</td>\n",
       "      <td>0.901749</td>\n",
       "      <td>0.805396</td>\n",
       "      <td>LR with L1 norm on unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.526932</td>\n",
       "      <td>0.856532</td>\n",
       "      <td>0.708818</td>\n",
       "      <td>LR with L2 norm on unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.711519</td>\n",
       "      <td>0.902080</td>\n",
       "      <td>0.803824</td>\n",
       "      <td>LR with L1 norm on digram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.676821</td>\n",
       "      <td>0.864366</td>\n",
       "      <td>0.708080</td>\n",
       "      <td>LR with L2 norm on digram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.025620</td>\n",
       "      <td>0.894869</td>\n",
       "      <td>0.773262</td>\n",
       "      <td>LR with L1 norm on trigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84.728667</td>\n",
       "      <td>0.854472</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>LR with L2 norm on trigram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall                       model\n",
       "0  89.570305  0.901749  0.805396  LR with L1 norm on unigram\n",
       "1  84.526932  0.856532  0.708818  LR with L2 norm on unigram\n",
       "2  89.711519  0.902080  0.803824   LR with L1 norm on digram\n",
       "3  85.676821  0.864366  0.708080   LR with L2 norm on digram\n",
       "4  89.025620  0.894869  0.773262  LR with L1 norm on trigram\n",
       "5  84.728667  0.854472  0.691336  LR with L2 norm on trigram"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags weighted by Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tags = []\n",
    "for t in all_tweets:\n",
    "    tokens = basic_tokenize(preprocessing(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_pos_tags(param,string):\n",
    "    pos_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=param,\n",
    "    stop_words=None,\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None,\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.75,\n",
    "    )\n",
    "    pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "    X = pd.DataFrame(pos)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    string1 = 'LR with L1 norm on ' + string\n",
    "    LR_l1(X_train,X_test,y_train,y_test,string1)\n",
    "    string2  = 'LR with L2 norm on ' + string\n",
    "    LR_l2(X_train,X_test,y_train,y_test,string2)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []\n",
    "tfidf_pos1=tf_idf_pos_tags((1,1),'unigram')\n",
    "tfidf_pos2=tf_idf_pos_tags((1,2),'digram')\n",
    "tfidf_pos3=tf_idf_pos_tags((1,3),'trigram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.186605</td>\n",
       "      <td>0.688983</td>\n",
       "      <td>0.357679</td>\n",
       "      <td>LR with L1 norm on unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.791003</td>\n",
       "      <td>0.536009</td>\n",
       "      <td>0.398268</td>\n",
       "      <td>LR with L2 norm on unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.892677</td>\n",
       "      <td>0.712052</td>\n",
       "      <td>0.376555</td>\n",
       "      <td>LR with L1 norm on digram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.877547</td>\n",
       "      <td>0.569286</td>\n",
       "      <td>0.435831</td>\n",
       "      <td>LR with L2 norm on digram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.912851</td>\n",
       "      <td>0.717261</td>\n",
       "      <td>0.382188</td>\n",
       "      <td>LR with L1 norm on trigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.275368</td>\n",
       "      <td>0.602912</td>\n",
       "      <td>0.412881</td>\n",
       "      <td>LR with L2 norm on trigram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall                       model\n",
       "0  75.186605  0.688983  0.357679  LR with L1 norm on unigram\n",
       "1  47.791003  0.536009  0.398268  LR with L2 norm on unigram\n",
       "2  75.892677  0.712052  0.376555   LR with L1 norm on digram\n",
       "3  50.877547  0.569286  0.435831   LR with L2 norm on digram\n",
       "4  75.912851  0.717261  0.382188  LR with L1 norm on trigram\n",
       "5  55.275368  0.602912  0.412881  LR with L2 norm on trigram"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocessing(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words)\n",
    "    num_chars = sum(len(w) for w in words)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet)\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n",
    "                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = get_feature_array(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(feats)\n",
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "string1 = 'LR with L1 norm'\n",
    "LR_l1(X_train,X_test,y_train,y_test,string1)\n",
    "string2  = 'LR with L2 norm'\n",
    "LR_l2(X_train,X_test,y_train,y_test,string2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.912851</td>\n",
       "      <td>0.748859</td>\n",
       "      <td>0.488027</td>\n",
       "      <td>LR with L1 norm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.445632</td>\n",
       "      <td>0.669407</td>\n",
       "      <td>0.542719</td>\n",
       "      <td>LR with L2 norm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall            model\n",
       "0  75.912851  0.748859  0.488027  LR with L1 norm\n",
       "1  63.445632  0.669407  0.542719  LR with L2 norm"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all the three features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = np.concatenate([tfidf2,tfidf_pos2,feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans = {}\n",
    "ans['model'] = []\n",
    "ans['F1-score'] = []\n",
    "ans['Recall'] = []\n",
    "ans['Accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = fd['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"LR_l1 using basic features\"\n",
    "LR_l1(X_train,X_test,y_train,y_test,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"LR_l2 using basic features\"\n",
    "LR_l2(X_train,X_test,y_train,y_test,string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.913254</td>\n",
       "      <td>0.902852</td>\n",
       "      <td>0.790027</td>\n",
       "      <td>LR_l1 using basic features</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.319548</td>\n",
       "      <td>0.891135</td>\n",
       "      <td>0.782296</td>\n",
       "      <td>LR_l2 using basic features</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  F1-score    Recall                       model\n",
       "0  89.913254  0.902852  0.790027  LR_l1 using basic features\n",
       "1  88.319548  0.891135  0.782296  LR_l2 using basic features"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ans)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
