{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% bash\n",
    "# set -e\n",
    "# CLASSPATH=\"lib:lib/stanford-parser/stanford-parser.jar:lib/stanford-parser/stanford-parser-3.5.1-models.jar\"\n",
    "# javac -cp $CLASSPATH lib/*.java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePattern(tweet, pattern):\n",
    "    r = re.findall(pattern, tweet)\n",
    "    for x in r:\n",
    "        tweet = re.sub(x, '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def make_dirs(dirs):\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "            \n",
    "def dependency_parse(filepath, cp='', tokenize=True):\n",
    "    print('\\nDependency parsing ' + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    filepre = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    tokpath = os.path.join(dirpath, filepre + '.toks')\n",
    "    parentpath = os.path.join(dirpath, filepre + '.parents')\n",
    "    relpath = os.path.join(dirpath, filepre + '.rels')\n",
    "    tokenize_flag = '-tokenize - ' if tokenize else ''\n",
    "    cmd = ('java -cp %s DependencyParse -tokpath %s -parentpath %s -relpath %s %s < %s'\n",
    "           % (cp, tokpath, parentpath, relpath, tokenize_flag, filepath))\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "def constituency_parse(filepath, cp='', tokenize=True):\n",
    "    print('\\nConst parsing ' + filepath)\n",
    "    dirpath = os.path.dirname(filepath)\n",
    "    filepre = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    tokpath = os.path.join(dirpath, filepre + '.toks')\n",
    "    parentpath = os.path.join(dirpath, filepre + '.cparents')\n",
    "    tokenize_flag = '-tokenize - ' if tokenize else ''\n",
    "    cmd = ('java -cp %s ConstituencyParse -tokpath %s -parentpath %s %s < %s'\n",
    "           % (cp, tokpath, parentpath, tokenize_flag, filepath))\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "def build_vocab(filepaths, dst_path, lowercase=True):\n",
    "    vocab = set()\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath) as f:\n",
    "            for line in f:\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                vocab |= set(line.split())\n",
    "    with open(dst_path, 'w') as f:\n",
    "        for w in sorted(vocab):\n",
    "            f.write(w + '\\n')\n",
    "\n",
    "\n",
    "def split(filepath, dst_dir):\n",
    "    with open(filepath) as datafile, \\\n",
    "            open(os.path.join(dst_dir, 'tweets.txt'), 'w') as tfile, \\\n",
    "            open(os.path.join(dst_dir, 'labels.txt'), 'w') as lfile:\n",
    "        \n",
    "        for line in csv.reader(datafile, delimiter=','):\n",
    "            label = line[-2]\n",
    "            tweet = line[-1].strip()\n",
    "            tweet = removePattern(tweet, \"@[\\w]*\")\n",
    "            tweet = tweet.replace(\"#\", \"\") # Removing '#' from hashtags\n",
    "            tweet = re.sub(\"[^a-zA-Z]+\", \" \", tweet) # Removing punctuation and special characters\n",
    "            tweet = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\"URL\", tweet)\n",
    "            tweet = re.sub(\" +\", \" \", tweet)\n",
    "            tweet = tweet.strip().lower()\n",
    "            if len(tweet) > 1:\n",
    "                tfile.write(tweet + \"\\n\")\n",
    "                lfile.write(label + \"\\n\")\n",
    "\n",
    "\n",
    "def parse(dirpath, cp=''):\n",
    "    dependency_parse(os.path.join(dirpath, 'tweets.txt'), cp=cp, tokenize=True)\n",
    "    constituency_parse(os.path.join(dirpath, 'tweets.txt'), cp=cp, tokenize=True)\n",
    "\n",
    "print('=' * 80)\n",
    "print('Preprocessing dataset')\n",
    "print('=' * 80)\n",
    "\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "#sick_dir = os.path.join(data_dir, 'sick')\n",
    "hate_dir = os.path.join(data_dir, 'hate-speech')\n",
    "lib_dir = os.path.join(base_dir, 'lib')\n",
    "train_dir = os.path.join(hate_dir, 'train')\n",
    "dev_dir = os.path.join(hate_dir, 'dev')\n",
    "test_dir = os.path.join(hate_dir, 'test')\n",
    "make_dirs([train_dir, dev_dir, test_dir])\n",
    "\n",
    "# java classpath for calling Stanford parser\n",
    "classpath = ':'.join([\n",
    "    lib_dir,\n",
    "    os.path.join(lib_dir, 'stanford-parser/stanford-parser.jar'),\n",
    "    os.path.join(lib_dir, 'stanford-parser/stanford-parser-3.5.1-models.jar')])\n",
    "\n",
    "# split into separate files\n",
    "split(os.path.join(hate_dir, 'hate_train.txt'), train_dir)\n",
    "split(os.path.join(hate_dir, 'hate_dev.txt'), dev_dir)\n",
    "split(os.path.join(hate_dir, 'hate_test.txt'), test_dir)\n",
    "\n",
    "# parse sentences\n",
    "parse(train_dir, cp=classpath)\n",
    "parse(dev_dir, cp=classpath)\n",
    "parse(test_dir, cp=classpath)\n",
    "\n",
    "# get vocabulary\n",
    "build_vocab(\n",
    "    glob.glob(os.path.join(hate_dir, '*/*.toks')),\n",
    "    os.path.join(hate_dir, 'vocab.txt'))\n",
    "build_vocab(\n",
    "    glob.glob(os.path.join(hate_dir, '*/*.toks')),\n",
    "    os.path.join(hate_dir, 'vocab-cased.txt'),\n",
    "    lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# IMPORT CONSTANTS\n",
    "from treelstm import Constants\n",
    "# NEURAL NETWORK MODULES/LAYERS\n",
    "from treelstm import SimilarityTreeLSTM\n",
    "# DATA HANDLING CLASSES\n",
    "from treelstm import Vocab\n",
    "# DATASET CLASS FOR hate DATASET\n",
    "from treelstm import HATEDataset\n",
    "# METRICS CLASS FOR EVALUATION\n",
    "from treelstm import Metrics\n",
    "# UTILITY FUNCTIONS\n",
    "from treelstm import utils\n",
    "# TRAIN AND TEST HELPER FUNCTIONS\n",
    "from treelstm import Trainer\n",
    "# CONFIG PARSER\n",
    "# from config import parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data = \"../../treelstm.pytorch/data/hate-speech\"\n",
    "    glove = \"../../treelstm.pytorch/data/glove\" \n",
    "    save = \"checkpoints/\"\n",
    "    expname = \"test\"\n",
    "    input_dim = 300\n",
    "    mem_dim = 150\n",
    "    hidden_dim = 250\n",
    "    num_classes = 3\n",
    "    freeze_embed = True\n",
    "    epochs = 3\n",
    "    batchsize = 64\n",
    "    lr = 0.01\n",
    "    wd = 1e-4\n",
    "    sparse = False\n",
    "    optim = \"adagrad\"\n",
    "    seed = 69\n",
    "    sparse = False\n",
    "    cuda = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,376] DEBUG:__main__:<__main__.Args object at 0x148681abd320>\n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:50,413] DEBUG:__main__:==> HATE vocabulary size : 23845 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,595] DEBUG:__main__:==> Size of train data   : 19752 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,767] DEBUG:__main__:==> Size of dev data     : 2510 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "[2019-11-10 16:34:54,950] DEBUG:__main__:==> Size of test data    : 2519 \n",
      "Training epoch 1: 100%|██████████| 19752/19752 [06:17<00:00, 52.27it/s]\n",
      "Testing epoch  1: 100%|██████████| 19752/19752 [03:07<00:00, 105.16it/s]\n",
      "Testing epoch  1: 100%|██████████| 2510/2510 [00:23<00:00, 107.80it/s]\n",
      "Testing epoch  1: 100%|██████████| 2519/2519 [00:22<00:00, 126.96it/s]\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,638] INFO:__main__:==> Epoch 0, Train \tLoss: 0.17337938739113143\tAccuracy: 0.8991494532199271\tPearson: 0.6977105736732483\tMSE: 0.1166464164853096\tF1: 0.1166464164853096\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,662] INFO:__main__:==> Epoch 0, Dev \tLoss: 0.15508964764611669\tAccuracy: 0.9203187250996016\tPearson: 0.7084940075874329\tMSE: 0.10717131197452545\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "[2019-11-10 16:45:06,689] INFO:__main__:==> Epoch 0, Test \tLoss: 0.1684110586813426\tAccuracy: 0.9067090115125049\tPearson: 0.6189954280853271\tMSE: 0.12187375873327255\t Recall: 0.5926698026847932\t F1: 0.8844884184839484\n",
      "Training epoch 2: 100%|██████████| 19752/19752 [06:14<00:00, 52.71it/s]\n",
      "Testing epoch  2: 100%|██████████| 19752/19752 [03:05<00:00, 106.75it/s]\n",
      "Testing epoch  2: 100%|██████████| 2510/2510 [00:22<00:00, 109.21it/s]\n",
      "Testing epoch  2: 100%|██████████| 2519/2519 [00:22<00:00, 126.78it/s]\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,036] INFO:__main__:==> Epoch 1, Train \tLoss: 0.14656576704690355\tAccuracy: 0.9116545159983799\tPearson: 0.7332491874694824\tMSE: 0.1083940863609314\tF1: 0.1083940863609314\n",
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n",
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n",
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n",
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n",
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-11-10 16:55:12,060] INFO:__main__:==> Epoch 1, Dev \tLoss: 0.1426872403385311\tAccuracy: 0.9262948207171314\tPearson: 0.7287521362304688\tMSE: 0.10358566045761108\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "[2019-11-10 16:55:12,088] INFO:__main__:==> Epoch 1, Test \tLoss: 0.15159676437082098\tAccuracy: 0.9106788408098452\tPearson: 0.6482273936271667\tMSE: 0.11909487843513489\t Recall: 0.6413503605948032\t F1: 0.8968644035605734\n",
      "Training epoch 3: 100%|██████████| 19752/19752 [06:14<00:00, 49.50it/s]\n",
      "Testing epoch  3: 100%|██████████| 19752/19752 [03:02<00:00, 108.24it/s]\n",
      "Testing epoch  3: 100%|██████████| 2510/2510 [00:23<00:00, 108.62it/s]\n",
      "Testing epoch  3: 100%|██████████| 2519/2519 [00:22<00:00, 126.51it/s]\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,918] INFO:__main__:==> Epoch 2, Train \tLoss: 0.1362129639623226\tAccuracy: 0.9179829890643986\tPearson: 0.7532545924186707\tMSE: 0.10191372781991959\tF1: 0.10191372781991959\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,939] INFO:__main__:==> Epoch 2, Dev \tLoss: 0.1414752534270157\tAccuracy: 0.9298804780876494\tPearson: 0.7536225318908691\tMSE: 0.09521912038326263\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n",
      "[2019-11-10 17:05:14,963] INFO:__main__:==> Epoch 2, Test \tLoss: 0.15141594951530943\tAccuracy: 0.9118697895990472\tPearson: 0.6637300848960876\tMSE: 0.11552203446626663\t Recall: 0.659951454799201\t F1: 0.9009858674253864\n"
     ]
    }
   ],
   "source": [
    "# MAIN BLOCK\n",
    "# args = parse_args()\n",
    "# global logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s:%(name)s:%(message)s\")\n",
    "# file logger\n",
    "fh = logging.FileHandler(os.path.join(args.save, args.expname)+'.log', mode='w')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "# console logger\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "# argument validation\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
    "if args.sparse and args.wd != 0:\n",
    "    logger.error('Sparsity and weight decay are incompatible, pick one!')\n",
    "    exit()\n",
    "logger.debug(args)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "if not os.path.exists(args.save):\n",
    "    os.makedirs(args.save)\n",
    "\n",
    "train_dir = os.path.join(args.data, 'train/')\n",
    "dev_dir = os.path.join(args.data, 'dev/')\n",
    "test_dir = os.path.join(args.data, 'test/')\n",
    "\n",
    "# write unique words from all token files\n",
    "hate_vocab_file = os.path.join(args.data, 'hate.vocab')\n",
    "if not os.path.isfile(hate_vocab_file):\n",
    "    #token_files_b = [os.path.join(split, 'b.toks') for split in [train_dir, dev_dir, test_dir]]\n",
    "    #token_files_a = [os.path.join(split, 'a.toks') for split in [train_dir, dev_dir, test_dir]]\n",
    "    token_files = [os.path.join(split, 'tweets.toks') for split in [train_dir, dev_dir, test_dir]]\n",
    "    #token_files = token_files_a + token_files_b\n",
    "    hate_vocab_file = os.path.join(args.data, 'hate.vocab')\n",
    "    utils.build_vocab(token_files, hate_vocab_file)\n",
    "\n",
    "# get vocab object from vocab file previously written\n",
    "vocab = Vocab(filename=hate_vocab_file,\n",
    "              data=[Constants.PAD_WORD, Constants.UNK_WORD,\n",
    "                    Constants.BOS_WORD, Constants.EOS_WORD])\n",
    "logger.debug('==> HATE vocabulary size : %d ' % vocab.size())\n",
    "\n",
    "# load HATE dataset splits\n",
    "train_file = os.path.join(args.data, 'hate_train.pth')\n",
    "if os.path.isfile(train_file):\n",
    "    train_dataset = torch.load(train_file)\n",
    "else:\n",
    "    train_dataset = HATEDataset(train_dir, vocab, args.num_classes)\n",
    "    torch.save(train_dataset, train_file)\n",
    "logger.debug('==> Size of train data   : %d ' % len(train_dataset))\n",
    "dev_file = os.path.join(args.data, 'hate_dev.pth')\n",
    "if os.path.isfile(dev_file):\n",
    "    dev_dataset = torch.load(dev_file)\n",
    "else:\n",
    "    dev_dataset = HATEDataset(dev_dir, vocab, args.num_classes)\n",
    "    torch.save(dev_dataset, dev_file)\n",
    "logger.debug('==> Size of dev data     : %d ' % len(dev_dataset))\n",
    "test_file = os.path.join(args.data, 'hate_test.pth')\n",
    "if os.path.isfile(test_file):\n",
    "    test_dataset = torch.load(test_file)\n",
    "else:\n",
    "    test_dataset = HATEDataset(test_dir, vocab, args.num_classes)\n",
    "    torch.save(test_dataset, test_file)\n",
    "logger.debug('==> Size of test data    : %d ' % len(test_dataset))\n",
    "\n",
    "# initialize model, criterion/loss_function, optimizer\n",
    "model = SimilarityTreeLSTM(\n",
    "    vocab.size(),\n",
    "    args.input_dim,\n",
    "    args.mem_dim,\n",
    "    args.hidden_dim,\n",
    "    args.num_classes,\n",
    "    args.sparse,\n",
    "    args.freeze_embed)\n",
    "#criterion = nn.KLDivLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# for words common to dataset vocab and GLOVE, use GLOVE vectors\n",
    "# for other words in dataset vocab, use random normal vectors\n",
    "emb_file = os.path.join(args.data, 'hate_embed.pth')\n",
    "if os.path.isfile(emb_file):\n",
    "    emb = torch.load(emb_file)\n",
    "else:\n",
    "    # load glove embeddings and vocab\n",
    "    glove_vocab, glove_emb = utils.load_word_vectors(\n",
    "        os.path.join(args.glove, 'glove.840B.300d'))\n",
    "    logger.debug('==> GLOVE vocabulary size: %d ' % glove_vocab.size())\n",
    "    emb = torch.zeros(vocab.size(), glove_emb.size(1), dtype=torch.float, device=device)\n",
    "    emb.normal_(0, 0.05)\n",
    "    # zero out the embeddings for padding and other special words if they are absent in vocab\n",
    "    for idx, item in enumerate([Constants.PAD_WORD, Constants.UNK_WORD,\n",
    "                                Constants.BOS_WORD, Constants.EOS_WORD]):\n",
    "        emb[idx].zero_()\n",
    "    for word in vocab.labelToIdx.keys():\n",
    "        if glove_vocab.getIndex(word):\n",
    "            emb[vocab.getIndex(word)] = glove_emb[glove_vocab.getIndex(word)]\n",
    "    torch.save(emb, emb_file)\n",
    "# plug these into embedding matrix inside model\n",
    "model.emb.weight.data.copy_(emb)\n",
    "\n",
    "model.to(device), criterion.to(device)\n",
    "if args.optim == 'adam':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                  model.parameters()), lr=args.lr, weight_decay=args.wd)\n",
    "elif args.optim == 'adagrad':\n",
    "    optimizer = optim.Adagrad(filter(lambda p: p.requires_grad,\n",
    "                                     model.parameters()), lr=args.lr, weight_decay=args.wd)\n",
    "elif args.optim == 'sgd':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad,\n",
    "                                 model.parameters()), lr=args.lr, weight_decay=args.wd)\n",
    "metrics = Metrics(args.num_classes)\n",
    "\n",
    "# create trainer object for training and testing\n",
    "trainer = Trainer(args, model, criterion, optimizer, device)\n",
    "\n",
    "best = -float('inf')\n",
    "for epoch in range(args.epochs):\n",
    "    train_loss = trainer.train(train_dataset)\n",
    "    train_loss, train_pred = trainer.test(train_dataset)\n",
    "    dev_loss, dev_pred = trainer.test(dev_dataset)\n",
    "    test_loss, test_pred = trainer.test(test_dataset)\n",
    "\n",
    "    train_acc = metrics.accuracy(train_pred, train_dataset.labels)\n",
    "    train_pearson = metrics.pearson(train_pred, train_dataset.labels)\n",
    "    train_mse = metrics.mse(train_pred, train_dataset.labels)\n",
    "    train_f1 = metrics.mse(train_pred, train_dataset.labels)\n",
    "    logger.info('==> Epoch {}, Train \\tLoss: {}\\tAccuracy: {}\\tPearson: {}\\tMSE: {}\\tF1: {}'.format(\n",
    "        epoch, train_loss, train_acc, train_pearson, train_mse,train_f1))\n",
    "    dev_acc = metrics.accuracy(dev_pred, dev_dataset.labels)\n",
    "    dev_pearson = metrics.pearson(dev_pred, dev_dataset.labels)\n",
    "    dev_mse = metrics.mse(dev_pred, dev_dataset.labels)\n",
    "    logger.info('==> Epoch {}, Dev \\tLoss: {}\\tAccuracy: {}\\tPearson: {}\\tMSE: {}'.format(\n",
    "        epoch, dev_loss, dev_acc, dev_pearson, dev_mse))\n",
    "    test_acc = metrics.accuracy(test_pred, test_dataset.labels)\n",
    "    test_pearson = metrics.pearson(test_pred, test_dataset.labels)\n",
    "    test_mse = metrics.mse(test_pred, test_dataset.labels)\n",
    "    test_rec = metrics.recall(test_pred, test_dataset.labels)\n",
    "    test_f1 = metrics.f1(test_pred, test_dataset.labels)\n",
    "    logger.info('==> Epoch {}, Test \\tLoss: {}\\tAccuracy: {}\\tPearson: {}\\tMSE: {}\\t Recall: {}\\t F1: {}'.format(\n",
    "        epoch, test_loss, test_acc, test_pearson, test_mse, test_rec, test_f1))\n",
    "\n",
    "#     if best < test_pearson:\n",
    "#         best = test_pearson\n",
    "#         checkpoint = {\n",
    "#             'model': trainer.model.state_dict(),\n",
    "#             'optim': trainer.optimizer,\n",
    "#             'pearson': test_pearson, 'mse': test_mse,\n",
    "#             'args': args, 'epoch': epoch\n",
    "#         }\n",
    "#         logger.debug('==> New optimum found, checkpointing everything now...')\n",
    "#         torch.save(checkpoint, '%s.pt' % os.path.join(args.save, args.expname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
